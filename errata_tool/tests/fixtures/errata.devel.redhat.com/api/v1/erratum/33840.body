{"who":{"user":{"email_address":null,"enabled":1,"id":3001865,"login_name":"kdreyer@redhat.com","preferences":{"default_filter_id":"1039","show_full_info":"1","color_scheme":""},"realname":"Ken Dreyer","receives_mail":true,"user_organization_id":142}},"params":{"format":"json","action":"show","controller":"api/v1/erratum","id":"33840"},"errata":{"rhba":{"actual_ship_date":"2018-09-26T18:17:30Z","assigned_to_id":3002003,"batch_id":null,"closed":0,"content_types":["rpm"],"contract":null,"created_at":"2018-05-03T22:40:06Z","current_state_index_id":233980,"current_tps_run":null,"deleted":0,"devel_responsibility_id":3,"doc_complete":1,"docs_responsibility_id":1,"embargo_undated":false,"filelist_changed":0,"filelist_locked":0,"fulladvisory":"RHBA-2018:2819-06","group_id":860,"id":33840,"is_batch_blocker":false,"is_brew":1,"is_valid":1,"issue_date":"2018-09-26T18:04:00Z","mailed":0,"manager_id":3001931,"old_advisory":"RHBA-2018:33840-06","old_delete_product":null,"package_owner_id":3001865,"priority":"normal","product_id":104,"publish_date_override":"2018-09-26T00:00:00Z","published":1,"published_shadow":0,"pushcount":1,"pushed":1,"qa_complete":1,"quality_responsibility_id":154,"rating":0,"release_date":null,"reporter_id":3001865,"request":0,"request_rcm_push_comment_id":null,"resolution":"","respin_count":10,"revision":6,"rhn_complete":0,"rhnqa":1,"rhnqa_shadow":0,"security_approved":null,"security_impact":"None","security_sla":"2018-10-01T00:00:00Z","severity":"normal","sign_requested":0,"state_machine_rule_set_id":null,"status":"SHIPPED_LIVE","status_updated_at":"2018-09-26T18:17:30Z","supports_multiple_product_destinations":true,"synopsis":"Red Hat Ceph Storage 3.1 Bug Fix update","text_only":false,"text_ready":0,"update_date":"2018-09-26T18:04:00Z","updated_at":"2018-09-26T18:25:11Z","embargo_date":null,"errata_id":2819,"publish_date":"2018-09-26T00:00:00Z"}},"content":{"content":{"crossref":"","cve":"","description":"Red Hat Ceph Storage is a scalable, open, software-defined storage platform\nthat combines the most stable version of the Ceph storage system with a\nCeph management platform, deployment utilities, and support services.\n\nBug Fix(es) and Enhancement(s):\n\nFor detailed information on changes in this release, see the Red Hat Ceph\nStorage 3.1 Release Notes available at:\n\nhttps://access.redhat.com/documentation/en-us/red_hat_ceph_storage/3.1/html-single/release_notes/","doc_review_due_at":null,"doc_reviewer_id":3000852,"errata_id":33840,"how_to_test":null,"id":31418,"keywords":"","multilib":null,"obsoletes":"","packages":null,"product_security_reviewer_id":null,"product_version_text":"","reference":"","revision_count":1,"solution":"Before applying this update, make sure all previously released errata\nrelevant to your system have been applied.\n\nFor details on how to apply this update, refer to:\n\nhttps://access.redhat.com/articles/11258","text_only_cpe":null,"topic":"Red Hat Ceph Storage 3.1 is now available.","updated_at":"2018-09-24T19:52:57Z"}},"diffs":{},"bugs":{"errata":{"rhba":{"actual_ship_date":"2018-09-26T18:17:30Z","assigned_to_id":3002003,"batch_id":null,"closed":0,"content_types":["rpm"],"contract":null,"created_at":"2018-05-03T22:40:06Z","current_state_index_id":233980,"current_tps_run":null,"deleted":0,"devel_responsibility_id":3,"doc_complete":1,"docs_responsibility_id":1,"embargo_undated":false,"filelist_changed":0,"filelist_locked":0,"fulladvisory":"RHBA-2018:2819-06","group_id":860,"id":33840,"is_batch_blocker":false,"is_brew":1,"is_valid":1,"issue_date":"2018-09-26T18:04:00Z","mailed":0,"manager_id":3001931,"old_advisory":"RHBA-2018:33840-06","old_delete_product":null,"package_owner_id":3001865,"priority":"normal","product_id":104,"publish_date_override":"2018-09-26T00:00:00Z","published":1,"published_shadow":0,"pushcount":1,"pushed":1,"qa_complete":1,"quality_responsibility_id":154,"rating":0,"release_date":null,"reporter_id":3001865,"request":0,"request_rcm_push_comment_id":null,"resolution":"","respin_count":10,"revision":6,"rhn_complete":0,"rhnqa":1,"rhnqa_shadow":0,"security_approved":null,"security_impact":"None","security_sla":"2018-10-01T00:00:00Z","severity":"normal","sign_requested":0,"state_machine_rule_set_id":null,"status":"SHIPPED_LIVE","status_updated_at":"2018-09-26T18:17:30Z","supports_multiple_product_destinations":true,"synopsis":"Red Hat Ceph Storage 3.1 Bug Fix update","text_only":false,"text_ready":0,"update_date":"2018-09-26T18:04:00Z","updated_at":"2018-09-26T18:25:11Z","embargo_date":null,"errata_id":2819,"publish_date":"2018-09-26T00:00:00Z"}},"id_field":"id","id_prefix":"bz:","type":"bugs","idsfixed":["1253486","1430536","1464945","1466956","1472868","1478598","1482739","1486830","1491250","1492242","1492342","1492589","1494256","1494987","1498303","1498323","1501380","1502021","1503607","1504291","1505400","1506102","1508451","1508460","1508611","1517912","1517915","1519159","1519835","1536795","1537035","1537390","1537445","1537505","1537737","1538207","1538330","1539663","1541520","1547999","1548563","1548564","1552101","1554281","1557465","1560022","1560101","1560554","1562220","1562386","1562388","1562389","1564084","1566194","1566513","1566514","1566643","1569192","1571353","1572032","1572722","1573657","1575829","1576204","1578140","1578142","1580300","1580497","1581164","1581564","1581571","1582181","1582281","1583020","1585023","1585029","1585031","1585139","1585192","1585239","1585307","1585750","1588093","1588812","1589146","1589545","1589792","1590275","1590628","1590746","1590831","1591074","1591822","1591873","1591877","1592497","1595896","1598339","1598522","1599859","1599873","1600071","1600613","1600943","1601068","1601325","1601580","1601949","1602158","1602327","1602785","1602882","1607970","1608863","1609030","1609427","1610213","1610220","1610997","1612854","1613155","1613626","1613963","1614286","1614287","1615329","1615330","1618678","1619098","1619167","1619189","1619255","1622210","1622505","1623417","1623737","1625191","1630447"],"bugs":[{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1253486,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"TestOnly","last_updated":"2018-09-26T18:17:48Z","package_id":39675,"pm_score":0,"priority":"medium","qa_whiteboard":"0","reconciled_at":"2018-09-26T18:25:20Z","release_notes":"","short_desc":"ffsb osd thrash test - osd/ReplicatedPG.cc: 2348: FAILED assert(0 == \"out of order op\")","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1430536,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"TestOnly, Triaged","last_updated":"2018-09-26T18:17:48Z","package_id":39676,"pm_score":0,"priority":"high","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:20Z","release_notes":".Improved Swift container ACL conformance has been added\r\n\r\nPreviously, {product} did not support certain ACL use cases, including setting of container ACLs whose subject is a Keystone project/tenant.\r\n\r\nWith this update of Ceph, many Swift container ACLs which were previously unsupported are now supported.","short_desc":"RGW: using swift CLI create/update container ACL failed with keystone","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text?","has_security_group":false,"id":1464945,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"FutureFeature, TestOnly","last_updated":"2018-09-26T18:17:48Z","package_id":39674,"pm_score":847,"priority":"urgent","qa_whiteboard":"2.25","reconciled_at":"2018-09-26T18:25:20Z","release_notes":"undefined","short_desc":"[RFE]: Streamlined disk replacement process","verified":"FailedQA","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,qe_test_coverage+,requires_doc_text+","has_security_group":false,"id":1466956,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"FutureFeature","last_updated":"2018-09-26T18:17:48Z","package_id":39676,"pm_score":2196,"priority":"high","qa_whiteboard":"CEPH-83572659","reconciled_at":"2018-09-26T18:25:20Z","release_notes":".Improvements to _radosgw-admin_ sync status commands\r\n\r\nWith this update of {product} a new `radosgw-admin bucket sync status` command has been added, as well as improvements to the existing `sync status` and `data sync status` commands.\r\n\r\nThese changes will make it easier to inspect the progress of multisite syncs.","short_desc":"[RFE] Improve RGW Multi-site radosgw-admin sync status and radosgw-admin bucket sync status commands","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"cee_cir+,ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1472868,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:17:48Z","package_id":39676,"pm_score":337,"priority":"low","qa_whiteboard":".75","reconciled_at":"2018-09-26T18:25:20Z","release_notes":".Quota stats cache is no longer invalid \r\n\r\nPreviously in {product}, quota values sometimes were not properly decremented. This could cause exceed errors when the quota was not actually exceeded.\r\n\r\nWith this update to Ceph, quota values are properly decremented and no incorrect errors are printed.","short_desc":"With RGWs configured in a load balancer, quota stats cache doesn't work","verified":"FailedQA","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1478598,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"TestOnly","last_updated":"2018-09-26T18:17:48Z","package_id":39681,"pm_score":0,"priority":"low","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:20Z","release_notes":"","short_desc":"'create' needs to allow filtering by uuid","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1482739,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"FutureFeature","last_updated":"2018-09-26T18:17:48Z","package_id":39676,"pm_score":880,"priority":"medium","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:20Z","release_notes":".Automated trimming of bucket index logs\r\n\r\nWhen multisite sync is used, all changes are logged in the bucket index. These logs can grow excessively large. They also are no longer needed once they have been processed by all peer zones.\r\n\r\nWith this update of {product}, the bucket index logs are automatically trimmed and do not grow beyond a reasonable size.","short_desc":"[RFE] RGW Bucket Index Trimming for MultisiteV2","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1486830,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"Triaged, ZStream","last_updated":"2018-10-02T20:20:09Z","package_id":39207,"pm_score":2520,"priority":"urgent","qa_whiteboard":"","reconciled_at":"2018-10-04T01:52:20Z","release_notes":".Containerized OSDs start after reboot\r\n\r\nPreviously, in a containerized environment, after rebooting Ceph storage nodes some OSDs might not have started. This was due to a race condition. The race condition was resolved and now all OSD nodes start properly after a reboot.","short_desc":"Some Containerized OSDs doesn't start after reboot","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1491250,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"TestOnly","last_updated":"2018-09-26T18:17:48Z","package_id":39681,"pm_score":0,"priority":"low","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:20Z","release_notes":"","short_desc":"implement `ceph-volume lvm list`","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,qe_test_coverage+","has_security_group":false,"id":1492242,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"Reopened","last_updated":"2018-09-26T18:17:48Z","package_id":39207,"pm_score":275,"priority":"low","qa_whiteboard":"Test Case ID: CEPH-9579","reconciled_at":"2018-09-26T18:25:20Z","release_notes":"","short_desc":"Playbook purge-cluster doesn't seem to clean up virtual disks; and fails on subsequent install.","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1492342,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:18:18Z","package_id":39656,"pm_score":330,"priority":"high","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:21Z","release_notes":".LUN resize on target side Ceph is now reflected on clients\r\n\r\nPreviously, when using the iSCSI gateway, resized Logical Unit Numbers (LUNs) were not immediately visible to initiators. This required a work around of restarting the iSCSI gateway after resizing a LUN to expose it to the initiators.\r\n\r\nWith this update to {product}, iSCSI initiators can now see a resized LUN immediately after rescan.","short_desc":"[iSCSI]: LUN resize on target side ceph not reflected on clients","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1492589,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-10-24T19:56:18Z","package_id":39676,"pm_score":600,"priority":"high","qa_whiteboard":"","reconciled_at":"2018-10-24T19:57:00Z","release_notes":".The Ceph Object Gateway requires applications to write sequentially\r\n\r\nThe Ceph Object Gateway requires applications to write sequentially from offset 0 to the end of a file. Attempting to write out of order causes the upload operation to fail. To work around this issue, use utilities like `cp`, `cat`, or `rsync` when copying files into NFS space. Always mount with the `sync` option.","short_desc":"[RGW:NFS]: Writing a large size file fails on the NFS mount","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1494256,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"TestOnly","last_updated":"2018-09-26T18:18:18Z","package_id":39673,"pm_score":930,"priority":"high","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:21Z","release_notes":".Load on MDS daemons is not always balanced fairly or evenly in multiple active MDS configurations\r\n\r\nPreviously, in certain cases, the MDS balancers offloaded too much metadata to another active daemon, or none at all.\r\n\r\nAs of this update to {product} this is no longer an issue as several balancer fixes and optimization have been made which address the issue.","short_desc":"[CephFS] Default load balancing is not sharing the load among 2 active MDSs","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1494987,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"TestOnly","last_updated":"2018-09-26T18:18:18Z","package_id":39673,"pm_score":330,"priority":"low","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:21Z","release_notes":".More accurate CephFS free space information\r\n\r\nThe CephFS kernel client now reports the same, more accurate free space information as the fuse client via the `df` command.","short_desc":"[CephFS]: Discrepancies b/w \"df \" output of kernel mount and fuse mount","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1498303,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:18:18Z","package_id":39207,"pm_score":1360,"priority":"high","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:21Z","release_notes":".Ceph Ansible no longer overwrites existing OSD partitions\r\n\r\nOn a OSD node reboot, it is possible that disk devices will get a different device path. For example, prior to restarting the OSD node, `/dev/sda` was an OSD, but after a reboot, the same OSD is now `/dev/sdb`. Previously, if no \"ceph\" partition was found on the disk, it was a valid OSD disk. With this release, if any partition is found on the disk, then the disk will not be used as an OSD.","short_desc":"Safety net option to prevent clobbering of existing partitions by OSD creation","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1498323,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"TestOnly","last_updated":"2018-09-26T18:18:18Z","package_id":39676,"pm_score":630,"priority":"medium","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:21Z","release_notes":"","short_desc":"[RHCeph 3.0]: s3a hadoop 2.8 integration test ITestS3AFileContextURI - Status Code: 403; Error Code: SignatureDoesNotMatch; Request ID: tx000000000000000000b6d-005928b901-104a-default","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,qe_test_coverage+,requires_doc_text+","has_security_group":false,"id":1501380,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:18:18Z","package_id":39676,"pm_score":660,"priority":"low","qa_whiteboard":"CEPH-11350","reconciled_at":"2018-09-26T18:25:21Z","release_notes":".Object compression works properly\r\n\r\nPreviously, when using zlib compression with Object Gateway, objects were not being compressed properly. The actual size and used size were listed as the same despite log messages saying compression was in use. This was due to the usage of smaller buffers. With this update to {product}, larger buffers are used and compression works as expected.","short_desc":"[RGW-Compression] - Compressed object(s) larger than initial size","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1502021,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:18:18Z","package_id":39656,"pm_score":225,"priority":"medium","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:21Z","release_notes":".The iSCSI gateway supports custom cluster names\r\n\r\nPreviously, the Ceph iSCSI gateway only worked with the default storage cluster name (`ceph`). In this release, the `rbd-target-gw` now supports arbitrary Ceph configuration file locations, which allows the use of storage clusters not named `ceph`. \r\n \r\nThe Ceph iSCSI gateway can be deployed using Ceph Ansible or using the command-line interface with a custom cluster name.","short_desc":"rbd-target-gw does not support cluster name different than 'ceph'","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1503607,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:18:18Z","package_id":39656,"pm_score":0,"priority":"medium","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:21Z","release_notes":"","short_desc":"tcmu-runner should support arbitrary Ceph cluster names","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,qe_test_coverage+,requires_doc_text+","has_security_group":false,"id":1504291,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"TestOnly","last_updated":"2018-09-26T18:18:18Z","package_id":39676,"pm_score":0,"priority":"medium","qa_whiteboard":"CEPH-83572736","reconciled_at":"2018-09-26T18:25:21Z","release_notes":".Marker objects no longer appear twice when listing objects\r\n\r\nPreviously, due to an error in processing, \"marker\" objects that were used to continue multi-segment listings were included incorrectly in the listing result. Consequently, such objects appeared twice in the listing output. With this update to {product}, objects are only listed once, as expected.","short_desc":"List Bucket incorrectly handles marker argument on versioned buckets","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1505400,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"TestOnly","last_updated":"2018-09-26T18:18:02Z","package_id":39676,"pm_score":330,"priority":"low","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:21Z","release_notes":".`delete_website_configuration` cannot be enabled by setting the bucket policy `DeleteBucketWebsite`\r\n\r\nIn the Ceph Object Gateway, a user cannot enable `delete_website_configuration` on a bucket even when a bucket policy has been written granting them `S3:DeleteBucketWebsite` permission.\r\n\r\nTo work around this issue, you can use other methods of permitting, for example, by using admin operations, by bucket owner, or by ACL.","short_desc":"[ rgw ]  s3: DeleteBucketWebsite fails with 403","verified":"Any","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1506102,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:18:02Z","package_id":39680,"pm_score":660,"priority":"high","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:21Z","release_notes":".The fixes for pg_num/pgp_num setting through the RESTful API\r\n\r\nPreviously, attempts to change `pgp_num` or `pg_num` via the RESTful API plugin failed. With this update to {product}, the API is able to change the `pgp_num` and `pg_num` parameter successfully.","short_desc":"[ceph-mgr] Few API commands do not work as expected.","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1508451,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:18:02Z","package_id":39656,"pm_score":0,"priority":"medium","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:21Z","release_notes":".Pools and images with hyphens ('-') are no longer rejected by the API\r\n\r\nPreviously, the iSCSI `gwcli` utility did not support hyphens in pool or image names. As such it was not possible to create a disk using a pool or image name that included hyphens (\"-\") by using the iSCSI `gwcli` utility.\r\n\r\nWith this update to {product}, the iSCSI `gwcli` utility correctly handles hyphens. As such creating a disk using a pool or image name with hyphens is now supported.","short_desc":"[ceph-iscsi-cli]: pools and images with hyphens ('-') are rejected by the API","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1508460,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:18:02Z","package_id":39207,"pm_score":1260,"priority":"high","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:21Z","release_notes":".Ansible no longer creates unused `systemd` unit files\r\n\r\nPreviously, when installing the Ceph Object Gateway by using the `ceph-ansible` utility, `ceph-ansible` created `systemd` unit files for the Ceph Object Gateway host corresponding to all Object Gateway instances located on other hosts. However, the only unit file that was active was the one that corresponded to the hostname of the Ceph Object Gateway. The others were not active and as such they did not cause problems. With this update of Ceph the other unit files are no longer created.","short_desc":"Changes to ceph-common handlers for RGWs","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1508611,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:18:02Z","package_id":39656,"pm_score":0,"priority":"low","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:21Z","release_notes":"","short_desc":"[tcmu-runner] annotate devices backed by solid state media","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1517912,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"FutureFeature, TestOnly","last_updated":"2018-09-26T18:18:02Z","package_id":39676,"pm_score":630,"priority":"medium","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:21Z","release_notes":".Admin socket command to invalidate cache \r\n\r\nTwo new admin socket commands to manipulate the cache were added to the `radosgw-admin` tool.\r\n\r\nThe `cache erase <objectname>` command flushes the given object from the cache.\r\n\r\nThe `cache zap` command erases the entire cache.\r\n\r\nThese commands can be used to help debug problems with the cache or provide a temporary workaround when an RGW node is holding stale information in the cache. Administrators can now flush any and all objects from the cache.","short_desc":"[RFE] Admin socket command to invalidate cache","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1517915,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"FutureFeature, TestOnly","last_updated":"2018-09-26T18:18:02Z","package_id":39676,"pm_score":630,"priority":"medium","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:21Z","release_notes":".New administrative sockets added for the `radosgw-admin` command to view the Object Gateway cache\r\n\r\nTwo new administrative sockets were added to the `radosgw-admin` command to view the contents of the Ceph Object Gateway cache.\r\n\r\nThe `cache list [string]` sub-command lists all objects in the cache. If the optional `string` is provided, it only matches those objects containing the string.\r\n\r\nThe `cache inspect <objectname>` sub-command prints detailed information about the object.\r\n\r\nThese commands can be used to help debug caching problems on any Ceph Object Gateway node.","short_desc":"[RFE] Admin socket command to view whats in rgw cache","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1519159,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"TestOnly","last_updated":"2018-09-26T18:18:02Z","package_id":39676,"pm_score":0,"priority":"low","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:21Z","release_notes":"","short_desc":"rgw: user stats increased after bucket reshard","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1519835,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-10-23T18:18:34Z","package_id":39207,"pm_score":776,"priority":"high","qa_whiteboard":"","reconciled_at":"2018-10-23T18:19:36Z","release_notes":"","short_desc":"[ceph-ansible] - RHEL - rhceph-3 cdn repos needs to be enabled instead of rhceph-2","verified":"FailedQA","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,qe_test_coverage+,requires_doc_text+","has_security_group":false,"id":1536795,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"TestOnly","last_updated":"2018-09-26T18:18:02Z","package_id":39676,"pm_score":1260,"priority":"high","qa_whiteboard":"CEPH-83571799","reconciled_at":"2018-09-26T18:25:21Z","release_notes":".Resharding a bucket that has ACLs set no longer alters the bucket ACL\r\n\r\nPreviously, in the Ceph Object Gateway (RGW), resharding a bucket with an access control list (ACL) set alters the bucket ACL. With this update to {product}, ACLs on a bucket are preserved even if they are resharded.","short_desc":"resharding doesn't seem to preserve bucket acls","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1537035,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:19:40Z","package_id":42766,"pm_score":930,"priority":"high","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:21Z","release_notes":".The _Ceph-pools_ Dashboard no longer displays previously deleted pools\r\n\r\nPreviously in the _Red Hat Ceph Storage Dashboard_, the _Ceph-pools_ Dashboard continued to reflect pools which were deleted from the Ceph Storage Cluster. With this update to Ceph they are no longer shown after being deleted.","short_desc":"[cephmetrics] Ceph-Pools: displays pools which are deleted.","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1537390,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:19:40Z","package_id":42766,"pm_score":1260,"priority":"high","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:21Z","release_notes":".Installation of {product} Dashboard with a non-default password no longer fails\r\n\r\nPreviously, the Red Hat Storage Dashboard (cephmetrics) could only be deployed with the default password. To use a different password it had to be changed in the Web UI afterwards.\r\n\r\nWith this update to {product} you can now set the Red Hat Ceph Storage Dashboard admin username and password using Ansible variables `grafana.admin_user` and `grafana.admin_password`.\r\n\r\nFor an example of how to set these variables, see the `group_vars/all.yml.sample` file.","short_desc":"[cephmetrics] Installation of cephmetrics with non-default password fails","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1537445,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"FutureFeature","last_updated":"2018-10-01T13:43:12Z","package_id":42766,"pm_score":3330,"priority":"medium","qa_whiteboard":"","reconciled_at":"2018-10-04T01:33:20Z","release_notes":".Installation of {product} Dashboard using the `ansible` user\r\n\r\nPreviously, installing {product} Dashboard (cephmetrics) with Ansible required root access. Traditionally, Ansible uses passwordless ssh and sudo with a regular user to install and make changes to systems. In this release, the {product} Dashboard can be installed with `ansible` using a regular user. For more information on the {product} Dashboard, see the link:{admin-guide}#installing-the-red-hat-ceph-storage-dashboard-management[Administration Guide].","short_desc":"[RFE] Support cephmetrics installation using the ansible user","verified":"FailedQA","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1537505,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:19:40Z","package_id":42766,"pm_score":330,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:21Z","release_notes":".OSD ids in 'Filestore OSD latencies' are no longer repeated\r\n\r\nPreviously, after rebooting OSDs, on the Red Hat Storage Dashboard page _Ceph OSD Information_ the OSD IDs were repeated in the section _Filestore OSD Latencies_. \r\n\r\nWith this update to {product} the OSD IDs are no longer repeated on reboot of an OSD node in the _Ceph OSD Information_ dashboard. This was fixed as a part of a redesign of the underlying data reporting.","short_desc":"[cephmetrics] Ceph OSD Information: OSD ids in 'Filestore OSD latencies' are repeated","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1537737,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:19:40Z","package_id":39676,"pm_score":1930,"priority":"high","qa_whiteboard":".75","reconciled_at":"2018-09-26T18:25:21Z","release_notes":".Intermittent HTTP error code 409 no longer occurs with compression enabled\r\n\r\nPreviously, HTTP error codes could be encountered due to EEXIST being incorrectly handled in `RGWPutObj::execute()` in a special case. This caused the PUT operation to be incorrectly failed to the client, when it should have been retried. In this update to {product}, the EEXIST condition handling has been corrected and this issue no longer occurs.","short_desc":"Intermittent http_status=409 with op status=-17 on ceph rgw with compression enabled","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1538207,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"FutureFeature","last_updated":"2018-09-26T18:19:40Z","package_id":39674,"pm_score":0,"priority":"low","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:21Z","release_notes":"","short_desc":"[RFE] Improve CRUSHTOOL to be able to modify CRUSH map as the ceph osd crush commands can","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1538330,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:19:40Z","package_id":39207,"pm_score":630,"priority":"medium","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:21Z","release_notes":"","short_desc":"ceph-ansible will fail silently for non-collocated configurations where journal disk is not present","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1539663,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"FutureFeature","last_updated":"2018-09-26T18:19:40Z","package_id":42766,"pm_score":1030,"priority":"medium","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:21Z","release_notes":".The {product} Dashboard displays the amount of used and available RAM on the storage cluster nodes\r\n\r\nPreviously, there was no way to view the actual memory usage on  cluster nodes from the _Red Hat Ceph Storage Dashboard_. With this update to {product}, a memory usage graph has been added to the _OSD Node Detail_ dashboard.","short_desc":"[RFE] Show the amount of RAM used/avail on RHCS nodes, in the Ceph Storage Dashboard","verified":"FailedQA","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1541520,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:19:40Z","package_id":39207,"pm_score":0,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:21Z","release_notes":"","short_desc":"ceph-ansible w/ containers in opentack mode creates pools but ignores specified size","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"cee_cir+,ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,qe_test_coverage?,requires_doc_text+","has_security_group":false,"id":1547999,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":1,"is_security":0,"issuetrackers":"","keywords":"TestOnly","last_updated":"2018-10-23T18:01:13Z","package_id":39207,"pm_score":1030,"priority":"medium","qa_whiteboard":"","reconciled_at":"2018-10-23T18:02:32Z","release_notes":".Purging a containerized Ceph installation using NVMe disks no longer fails\r\n\r\nPreviously, when attempting to purge a containerized Ceph installation using NVME disks, the purge failed because of a typo in the Ansible playbook which missed identifying NVMe block devices. With this update of {product}, the issue is fixed.","short_desc":"[ceph-ansible]purge-docker-cluster.yml fails with NVME disks","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1548563,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"FutureFeature","last_updated":"2018-09-26T18:21:35Z","package_id":39676,"pm_score":1060,"priority":"low","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:21Z","release_notes":".Implementation of partial order bucket/container listing\r\n\r\nPreviously, list bucket/container operations always returned elements in a sorted order. This has high overhead with sharded bucket indexes. Some protocols can tolerate receiving elements in arbitrary order so this is now allowed. An example `curl` command using this new feature:\r\n\r\n`curl GET http://server:8080/tb1?allow-unordered=True`\r\n\r\nWith this update to {product}, unordered listing via Swift and S3 is supported.","short_desc":"[RFE] rgw: implement partial order bucket/container listing (perf)","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"cee_cir+,ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1548564,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"FutureFeature","last_updated":"2018-09-26T18:21:35Z","package_id":39676,"pm_score":1060,"priority":"low","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:21Z","release_notes":".Asynchronous Garbage Collection\r\n\r\nAn asynchronous mechanism for executing the Ceph Object Gateway garbage collection using the `librados` APIs has been introduced. The original garbage collection mechanism serialized all processing, and lagged behind applications in specific workloads. Garbage collection performance has been significantly improved, and can be tuned to specific site requirements.","short_desc":"[RFE] implement parallel async/mt garbage collection","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,qe_test_coverage+","has_security_group":false,"id":1552101,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"Reopened","last_updated":"2018-09-26T18:21:35Z","package_id":39207,"pm_score":5270,"priority":"urgent","qa_whiteboard":"CEPH-11110","reconciled_at":"2018-09-26T18:25:21Z","release_notes":"","short_desc":"OC deploy timeouts with 3ceph nodes - ceph fsid hangs","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1554281,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:21:35Z","package_id":42766,"pm_score":600,"priority":"high","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:21Z","release_notes":".The Prometheus plugin for the {product} Dashboard\r\n\r\nPreviously, the {product} Dashboard used `collectd` and Graphite for gathering and reporting on Ceph metrics. With this release, Prometheus is now used for data gathering and reporting, and provides querying capabilities. Also, Prometheus is much less resource intensive. See the {product} link:{admin-guide}#the-prometheus-plugin-for-red-hat-ceph-storage[Administration Guide] for more details on the Prometheus plugin.","short_desc":"[cephmetrics] Prometheus enablement","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1557465,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:21:35Z","package_id":39673,"pm_score":630,"priority":"medium","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:21Z","release_notes":"","short_desc":"[nfs-ganesha] convert to rados_ng RecoveryBackend","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1560022,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:21:35Z","package_id":39207,"pm_score":660,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:21Z","release_notes":"","short_desc":"The keyrings are not refreshed if the key caps are changed on update","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,qe_test_coverage+,requires_doc_text+","has_security_group":false,"id":1560101,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:21:35Z","package_id":39676,"pm_score":730,"priority":"high","qa_whiteboard":"CEPH-83572737","reconciled_at":"2018-09-26T18:25:21Z","release_notes":".RGW no longer spikes to 100% CPU usage with no op traffic\r\n\r\nPreviously in certain situations an infinite loop could be encountered in `rgw_get_system_obj()`. This could cause spikes in CPU usage. With this update to {product} this specific issue has been resolved.","short_desc":"RGW spinning at 100% CPU with no op traffic","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,needinfo?,pm_ack+,qa_ack+,qe_test_coverage+","has_security_group":false,"id":1560554,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":1,"is_security":0,"issuetrackers":"","keywords":"TestOnly","last_updated":"2018-09-26T18:21:35Z","package_id":39474,"pm_score":2620,"priority":"urgent","qa_whiteboard":"CEPH-83572739","reconciled_at":"2018-09-26T18:25:21Z","release_notes":"","short_desc":"RGW Multi-site document offline resharding steps","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1562220,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"Reopened","last_updated":"2018-09-26T18:21:35Z","package_id":39207,"pm_score":630,"priority":"medium","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:21Z","release_notes":"","short_desc":"openstack_config.yml task \"assign rbd application to pool\" incorrectly assigns \"rbd\" to metrics, when it should be openstack_gnocchi","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1562386,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"CodeChange","last_updated":"2018-09-26T18:21:35Z","package_id":39656,"pm_score":0,"priority":"low","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:21Z","release_notes":"","short_desc":"Rebase tcmu-runner to 1.3.Z","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1562388,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"FutureFeature","last_updated":"2018-09-26T18:20:45Z","package_id":39656,"pm_score":630,"priority":"medium","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:21Z","release_notes":".The `max_data_area_mb` option is configurable per-LUN\r\n\r\nPreviously, the amount of memory the kernel used to pass SCSI command data to `tcmu-runner` was hard coded to 8MB. The hard coded limit was too small for many workloads and resulted in reduced throughput and/or TASK SET FULL errors filling initiator side logs. This can now be configured by setting the `max_data_area_mb` value with `gwcli`. Information on the new setting and command can be found in the {product} link:{block-dev-guide}#configuring_the_iscsi_target_using_the_command_line_interface[Block Device Guide].","short_desc":"[RFE] Allow 'max_data_area_mb' property to be configured per-LUN","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1562389,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"FutureFeature","last_updated":"2018-09-26T18:20:45Z","package_id":39656,"pm_score":300,"priority":"medium","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:21Z","release_notes":".iSCSI gateway command-line utility (`gwcli`) supports snapshot create, delete, and rollback capabilities \r\n\r\nPreviously, to manage the snapshots of RBD-backed LUN images the `rbd` command line utility was utilized for this purpose. The `gwcli` utility now includes built-in support for managing LUN snapshots. With this release, all snapshot related operations can now be handled directly within the `gwcli` utility.","short_desc":"[RFE] support snapshot create/delete/rollback within gwcli","verified":"FailedQA","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1564084,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:20:45Z","package_id":39656,"pm_score":300,"priority":"medium","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:21Z","release_notes":".Rebooting the Object Gateway no longer sees the `tcmu_rbd_lock_break` and `tcmu_notify_lock_lost` errors\r\n\r\nDuring failover/failback and initial device discovery with a multipath setup, `tcmu-runner` may need to take the lock away from another iSCSI gateway. When this happens the initiator may be sending IO to multiple paths at the same time, so multiple gateways will try to take the lock from each other. This can result in errors like:\r\n\r\n`Could not break lock from $locker_id (Err -16)`\r\n\r\nThese errors are expected temporarily while the failover/failback or device setup process is executing. However, `tcmu-runner` would sometimes not fully clean itself up after losing the lock, so the `-16` error would repeat and the gateway could not be used until it was rebooted. The `tcmu-runner` daemon has been changed to always reopen the device after losing the lock so the internal state is reinitialized before use.\r\n\r\nWith this update to {product}, failover/failback and device set up complete. Lock breaking errors with the error code `-16` should only temporarily be logged to the target logs.","short_desc":"[tcmu-runner] tcmu_rbd_lock_break and tcmu_notify_lock_lost errors on reboot of gw node","verified":"FailedQA","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text?","has_security_group":false,"id":1566194,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"CodeChange, FutureFeature","last_updated":"2018-09-26T18:20:45Z","package_id":39673,"pm_score":600,"priority":"high","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:21Z","release_notes":"undefined","short_desc":"[RFE] allow ceph_volume_client to create 'volumes' without namespace isolation","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1566513,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"CodeChange","last_updated":"2018-09-26T18:20:45Z","package_id":39656,"pm_score":0,"priority":"low","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:22Z","release_notes":"","short_desc":"Rebase python-rtslib, ceph-iscsi-config, and ceph-iscsi-cli packages","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1566514,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"FutureFeature","last_updated":"2018-09-26T18:20:45Z","package_id":39656,"pm_score":300,"priority":"medium","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:22Z","release_notes":".Disabling CHAP for iSCSI gateway authentication\r\n\r\nPreviously, CHAP authentication was required when using the Ceph iSCSI gateway. With this release, disabling CHAP authentication can be configured with the `gwcli` utility or with Ceph Ansible. However, mixing clients with CHAP enabled and disabled is not supported. All clients must either have CHAP enabled or disabled. If enabled, clients might have different CHAP credentials.","short_desc":"[RFE] Support no authentication for all clients from gwcli","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1566643,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:20:45Z","package_id":39656,"pm_score":0,"priority":"medium","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:22Z","release_notes":"","short_desc":"tcmu-runner systemd unit file should stop/restart ceph-iscsi-gw unit","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1569192,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"FutureFeature","last_updated":"2018-09-26T18:20:45Z","package_id":39674,"pm_score":630,"priority":"medium","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:22Z","release_notes":".Warnings about objects with too many omap entries\r\n\r\nWith this update to {product} warnings are displayed about pools which contain large omap objects. They can be seen in the output of `ceph health detail`. Information about the large objects in the pool are printed in the cluster logs. The settings which control when the warnings are printed are `osd_deep_scrub_large_omap_object_key_threshold` and `osd_deep_scrub_large_omap_object_value_sum_threshold`.","short_desc":"[RFE] osd: Warn about objects with too many omap entries","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1571353,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:20:45Z","package_id":18485,"pm_score":0,"priority":"medium","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:22Z","release_notes":".Rebase Ceph to version 12.2.5\r\n\r\n{product} 3.1 is now based on upstream Ceph Luminous 12.2.5.","short_desc":"rebase ceph to 12.2.5","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,qe_test_coverage+","has_security_group":false,"id":1572032,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:20:45Z","package_id":39207,"pm_score":660,"priority":"high","qa_whiteboard":"Test Case ID: CEPH-10880","reconciled_at":"2018-09-26T18:25:22Z","release_notes":"","short_desc":"[ceph-ansible] - rolling update not working on cluster intialised using iso","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1572722,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":1,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-10-23T23:07:01Z","package_id":39672,"pm_score":1260,"priority":"high","qa_whiteboard":"","reconciled_at":"2018-10-23T23:07:52Z","release_notes":".The `ceph-disk` utility defaults to BlueStore and when replacing an OSD, passing `--filestore` option is required\r\n \r\nPreviously, the `ceph-disk` utility used BlueStore as the default object store when creating OSDs. If the `--filestore` option was not used, then this caused problems in storage clusters using FileStore. In this release, the `ceph-disk` utility now defaults to FileStore as it had originally.","short_desc":"In RHCS 3 ceph-disk defaults to Bluestore and in OSD replacement passing --filestore is a must","verified":"FailedQA","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1573657,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":1,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:21:34Z","package_id":39676,"pm_score":1660,"priority":"high","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:22Z","release_notes":"","short_desc":"rgw: require --yes-i-really-mean-it to run radosgw-admin orphans find","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1575829,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"FutureFeature","last_updated":"2018-09-26T18:21:34Z","package_id":39207,"pm_score":660,"priority":"high","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:22Z","release_notes":".Support for iSCSI gateway upgrades through rolling updates\r\n\r\nPreviously, when using a Ceph iSCSI gateway node, `iscsi-gws` could not be updated by `ceph-ansible` during a rolling upgrade. With this update to {product}, `ceph-ansible` now supports upgrading `iscsi-gws` using the `rolling_update.yml` Ansible playbook.","short_desc":"[RFE] Support iscsi-gws upgrade through rolling update","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,qe_test_coverage+","has_security_group":false,"id":1576204,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:21:34Z","package_id":39674,"pm_score":0,"priority":"high","qa_whiteboard":"CEPH-83572697","reconciled_at":"2018-09-26T18:25:22Z","release_notes":"","short_desc":"Slow/Blocked requests no longer show details","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,qe_test_coverage+,requires_doc_text+","has_security_group":false,"id":1578140,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":1,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:21:34Z","package_id":39673,"pm_score":5460,"priority":"urgent","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:22Z","release_notes":".MDS no longer asserts \r\n\r\nPreviously, the Ceph Metadata Server (MDS) would sometimes assert and fail because client session imports would race with incoming client connections. With this update to {product}, MDS handles the race condition and continues normally.","short_desc":"[GSS][CephFS] MDS asserts with 'FAILED assert(session_map.count(s->info.inst.name) == 0)'","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1578142,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:21:34Z","package_id":39673,"pm_score":1660,"priority":"high","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:22Z","release_notes":".MDS no longer asserts while in starting/resolve state\r\n\r\nPreviously, when increasing \"max_mds\" from \"1\" to \"2\", if the Metadata Server (MDS) daemon was in the starting/resolve state for a long period of time, then restarting the MDS daemon led to an assert. This caused the Ceph File System (CephFS) to enter a degraded state. With this update to {product}, the underlying issue has been fixed, and increasing \"max_mds\" no longer causes CephFS to enter a degraded state.","short_desc":"[cephfs]: MDS asserted while in Starting/resolve state","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1580300,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":1,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-10-23T18:49:01Z","package_id":39674,"pm_score":2860,"priority":"high","qa_whiteboard":"","reconciled_at":"2018-10-23T18:49:10Z","release_notes":".The Ceph OSD daemon terminates with a segmentation fault\r\n\r\nPreviously, a subtle race condition in the `ceph-osd` daemon could lead to the corruption of the `osd_health_metrics` data structure which results in corrupted data being sent to, and reported by, the Ceph Manager. This ultimately caused a segmentation fault. With this update to {product}, a lock is now acquired before modifying the `osd_health_metrics` data structure.","short_desc":"[CEE/SD][ceph-osd]ceph-osd segfaults with \"in thread 7f02ae07d700 thread_name:safe_timer\"","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1580497,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:21:34Z","package_id":39474,"pm_score":2620,"priority":"urgent","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:22Z","release_notes":"Previously, internal RADOS Gateway (RGW) multi-site sync logic behaved incorrectly in some scenarios when attempting to sync containers with S3 object versioning enabled\u2014in particular, when a new object upload was followed immediately by an attribute or ACL setting operation. Objects in versioning-enabled containers would fail to sync in some scenarios\u2014for example, when using \"s3cmd sync\" to mirror a filesystem directory. With this update, RGW multi-site replication logic has been corrected for the known failure cases.","short_desc":"Fixing RGW Multi-site for versioned buckets/objects","verified":"FailedQA","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1581164,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"Automation, Regression","last_updated":"2018-09-26T18:21:34Z","package_id":39207,"pm_score":1560,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:22Z","release_notes":"","short_desc":"[Ceph-ansible] ansible-playbook fails due to new mandatory pg_num parameter","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1581564,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"Regression, Reopened","last_updated":"2018-09-26T18:21:34Z","package_id":39674,"pm_score":2220,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:22Z","release_notes":"","short_desc":"rolling upgrade to 3.1 from 3.0z3 fails","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1581571,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:22:05Z","package_id":39207,"pm_score":0,"priority":"medium","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:22Z","release_notes":"","short_desc":"Skip GPT header creation for lvm osd scenario","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1582181,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:22:05Z","package_id":39207,"pm_score":0,"priority":"medium","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:22Z","release_notes":"","short_desc":"LVM lvcreate fails if the disk already has a GPT header","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1582281,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:22:05Z","package_id":39207,"pm_score":0,"priority":"medium","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:22Z","release_notes":"","short_desc":"facts are gathered 2 times for client nodes on containerized deployment","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"automate_bug+,ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1583020,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"Automation, Regression","last_updated":"2018-09-26T18:22:05Z","package_id":39207,"pm_score":1230,"priority":"low","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:22Z","release_notes":"","short_desc":"[Ceph-ansible] 3 mdss are getting active after fresh cluster setup","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1585023,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:22:05Z","package_id":39673,"pm_score":1260,"priority":"high","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:22Z","release_notes":"","short_desc":"[CephFS]: MDS assert, ceph-12.2.1/src/mds/MDCache.cc: 5080: FAILED assert(isolated_inodes.empty())","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1585029,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:22:05Z","package_id":39673,"pm_score":930,"priority":"high","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:22Z","release_notes":".Client I/O sometimes fails for CephFS FUSE clients\r\n\r\nClient I/O sometimes failed for Ceph File System (CephFS) as a File System in User Space (FUSE) client with the error `transport endpoint shutdown` due to an assert in the FUSE service. With this update to {product}, the issue is resolved.","short_desc":"[CephFS]: Client IO's hung Fuse service asserted with error FAILED assert(oset.objects.empty()","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1585031,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":1,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:22:05Z","package_id":39673,"pm_score":1660,"priority":"high","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:22Z","release_notes":".Client fails with segmentation fault and \"Transport endpoint is not connected\"\r\n\r\nPreviously, the the Ceph File System (CephFS) client invalidated an iterator to its capabilities while trimming its cache. This caused the client to suffer a segmentation fault. With this update to {product}, the client prevents the iterator from being invalidating, and the client continues normally.","short_desc":"[CEE/SD][cephfs-fuse] client failing with Segmentation fault and getting Transport endpoint is not connected","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1585139,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:22:05Z","package_id":39207,"pm_score":2520,"priority":"urgent","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:22Z","release_notes":"","short_desc":"osp13 deployment fails - 'dict object' has no attribute 'caps' in roles/ceph-osd/tasks/openstack_config.yml","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1585192,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:22:05Z","package_id":39682,"pm_score":730,"priority":"medium","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:22Z","release_notes":"","short_desc":"entries_behind_master metric output but \"rbd mirror image status \" never reduces to zero.","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1585239,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-10-03T15:40:07Z","package_id":39474,"pm_score":1660,"priority":"high","qa_whiteboard":"","reconciled_at":"2018-10-04T02:01:24Z","release_notes":".Some versioned objects do not sync when uploaded with 's3cmd sync'\r\n\r\nOperations like `PutACL` that only modify object metadata do not generate a LINK_OLH entry in the bucket index log. When processed by multisite sync, these operations were skipped with the message `versioned object will be synced on link_olh`. Because of sync squashing, this caused the original LINK_OLH operation to be skipped as well, preventing the object version from syncing at all. With this update to {product} this issue no longer occurs.","short_desc":"Some versioned objects don't sync when uploaded with 's3cmd sync'","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,needinfo?,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1585307,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"FutureFeature","last_updated":"2018-09-26T18:23:09Z","package_id":39676,"pm_score":730,"priority":"medium","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:22Z","release_notes":".Relaxed region constraint enforcement\r\n\r\nIn {product} 3.x when using `s3cmd` and option `--region` with a zonegroup that does not exist an `InvalidLocationConstraint` error will be generated. This did not occur in Ceph 2.x because it did not have strict checking on the region. With this update Ceph 3.1 adds a new `rgw_relaxed_region_enforcement` boolean option to enable relaxed (non-enforcement of region constraint) behavior backward compatible with Ceph 2.x. The option defaults to False.","short_desc":"[RFE] RGW: Relaxed region constraint enforcement","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,needinfo?,pm_ack+,qa_ack+,qe_test_coverage+,requires_doc_text+","has_security_group":false,"id":1585750,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:23:09Z","package_id":39676,"pm_score":730,"priority":"medium","qa_whiteboard":"CEPH-83572812","reconciled_at":"2018-09-26T18:25:22Z","release_notes":".Cache entries now refresh as expected\r\n\r\nThe new time-based metadata cache entry expiration logic did not include logic to update the expiration time on already-cached entries being updated in place. Cache entries became permanently stale after expiration, leading to a performance regression as metadata objects were effectively not cached and always read from the cluster. To resolve this issue, in {product} 3.1, logic has been added to update the expiration time of cached entries when updated.","short_desc":"objects in cache never refresh after rgw_cache_expiry_interval","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1588093,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"Triaged","last_updated":"2018-09-26T18:23:09Z","package_id":39207,"pm_score":2520,"priority":"urgent","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:22Z","release_notes":".The OpenStack keys are copied to all Ceph Monitors\r\n\r\nWhen {product} was configured with `run_once: true` and  `inventory_hostname == groups.get(client_group_name) | first` it can cause a bug when the only node being run is not the first node in the group. In a deployment with a single client node the keyrings will not be created since the task can be skipped. With this release this situation no longer occurs and all the OpenStack keys are copied to the monitor nodes.","short_desc":"openstack_keys are not copied over to all MONs","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,needinfo?,pm_ack+,qa_ack+","has_security_group":false,"id":1588812,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:23:09Z","package_id":39676,"pm_score":330,"priority":"low","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:22Z","release_notes":"","short_desc":"RGW:  when using bucket request payer with boto3, NotImplemented error is seen.","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1589146,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:23:09Z","package_id":39207,"pm_score":930,"priority":"high","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:22Z","release_notes":"","short_desc":"Ceph-Ansible requires firewalld service to be enabled","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"automate_bug?,ceph-3.y+,devel_ack+,hot_fix_requested+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1589545,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":1,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:23:09Z","package_id":39474,"pm_score":1660,"priority":"high","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:22Z","release_notes":".RGW multisite does not sync all objects\r\n\r\nPreviously, in the Ceph Object Gateway multisite scenarios, an HTTP request to another gateway would not complete. Therefore, multisite sync would wait forever on the request and could not make further progress. With this update to {product}, a timeout to the \"libcurl\" request has been added, and HTTP requests that do not complete will time out and be retried, allowing multisite sync to continue.","short_desc":"[GSS] RGW multisite does not sync all objects","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1589792,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:23:09Z","package_id":39207,"pm_score":0,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:22Z","release_notes":"","short_desc":"ceph-osd: set 'openstack_keys_tmp' only when 'openstack_config' is defined.","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1590275,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:23:09Z","package_id":39207,"pm_score":0,"priority":"low","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:22Z","release_notes":"","short_desc":"Pool application tags are not applied unless supplied","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1590628,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:23:09Z","package_id":39207,"pm_score":0,"priority":"low","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:22Z","release_notes":"","short_desc":"ceph-ansible fails at the ceph-create-keys removal stage","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1590746,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:23:09Z","package_id":39207,"pm_score":2520,"priority":"urgent","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:22Z","release_notes":".The ceph-ansible utility removes the ceph-create-keys container from the same node where it was created.\r\n\r\nPreviously, the `ceph-ansible` utility did not always remove the `ceph-create-keys` container from the same node where it was created. Because of this, the deployment could fail with the message \"Error response from daemon: No such container: ceph-create-keys.\" With this update to {product}, `ceph-ansible` only tries to remove the container from the node where it was actually created, thus avoiding the error and not causing the deployment to fail.","short_desc":"ceph upgrade/deployment fails with \"Error response from daemon: No such container: ceph-create-keys\"","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1590831,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:23:34Z","package_id":39207,"pm_score":0,"priority":"medium","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:22Z","release_notes":"","short_desc":"enable multimds only on luminous","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1591074,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:23:34Z","package_id":39207,"pm_score":2520,"priority":"urgent","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:22Z","release_notes":".Support NVMe based bucket index pools\r\n\r\nPreviously, configuring Ceph to optimize storage on high speed NVMe or SATA SSDs when using Object Gateway was a completely manual process which required complicated LVM configuration.\r\n\r\nWith this release, the `ceph-ansible` package provides two new Ansible playbooks that facilitate setting up SSD storage using LVM to optimize performance when using Object Gateway. See the link:{object-gw-production}#using-nvme-with-lvm-optimally[Using NVMe with LVM Optimally] chapter in the {product} Object Gateway for Production Guide for more information.","short_desc":"Support NVMe based bucket index pools","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1591822,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"Improvement, Performance","last_updated":"2018-09-26T18:23:34Z","package_id":39676,"pm_score":930,"priority":"high","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:22Z","release_notes":".Default `rgw_thread_pool_size` value change to 512\r\n\r\nThe default `rgw_thread_pool_size` value changed from 100 to 512. This change accommodates larger workloads. Decrease this value for smaller workloads.","short_desc":"change default rgw_thread_pool_size to 512","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1591873,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:23:34Z","package_id":39674,"pm_score":1260,"priority":"high","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:22Z","release_notes":".The `filestore_merge_threshold` option default has changed\r\n\r\nSubdirectory merging has been disabled by default. The default value of the `filestore_merge_threshold` option has changed to -10 from 10. It has been observed to improve performance significantly on larger systems with a minimal performance impact to smaller systems. To take advantage of this performance increase set the `expected-num-objects` value when creating new data pools. See the link:{object-gw-production}#creating-a-data-pool-rgw-adv[Object Gateway for Production Guide] for more information.","short_desc":"Set default filestore_merge_threshold = -10","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1591877,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"CodeChange","last_updated":"2018-09-26T18:23:34Z","package_id":39676,"pm_score":1260,"priority":"high","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:22Z","release_notes":".Increased the default value for the `objecter_inflight_ops` option\r\n\r\nThe default value for the `objecter_inflight_ops` option was changed from 1024 to 24576. The original default value was insufficient to support a typical Object Gateway workload. With this enhancement, larger workloads are supported by default.","short_desc":"Set objecter_inflight_ops = 24576 for bulk RGW deployments","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"automate_bug?,ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,qe_test_coverage?,requires_doc_text+","has_security_group":false,"id":1592497,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:23:34Z","package_id":39674,"pm_score":1260,"priority":"high","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:22Z","release_notes":".High object counts can degrade IO performance\r\n\r\nThe overhead with directory merging on FileStore can degrade the client's IO performance for pools with high object counts.\r\n\r\nTo work around this issue, use the \u2018expected_num_objects\u2019 option during pool creation. Creating pools is described in the {product} link:{object-gw-production}#assembly-creating-data-placement-strategies-rgw-adv[Object Gateway for Production Guide].","short_desc":"cluster fill RGW workload performance degrades by 2/3rds during run","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1595896,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:23:34Z","package_id":39674,"pm_score":630,"priority":"medium","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:22Z","release_notes":"","short_desc":"osd pool create should warn when 'expect_num_objects' is being ignored","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1598339,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"Reopened","last_updated":"2018-09-26T18:23:34Z","package_id":42766,"pm_score":660,"priority":"urgent","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:22Z","release_notes":"","short_desc":"[cephmetrics] Prometheus based container ceph-metrics installation fails","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1598522,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:23:34Z","package_id":18485,"pm_score":330,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:22Z","release_notes":"","short_desc":"rook RPM pulls ceph-mon, ceph-mgr, ceph-osd into RHEL Tools repository","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1599859,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"CodeChange","last_updated":"2018-09-26T18:23:34Z","package_id":39674,"pm_score":1030,"priority":"medium","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:22Z","release_notes":".Reduced OSD memory usage\r\n\r\nBuffers from client operations were not being rebuilt, which was leading to unnecessary memory growth by an OSD process. Rebuilding the buffers has reduced the memory footprint for OSDs in Object Gateway workloads.","short_desc":"[Continuous OSD memory usage growth in a HEALTH_OK cluster] RGW workload makes OSD memory explode","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1599873,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:23:15Z","package_id":42766,"pm_score":0,"priority":"low","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:23Z","release_notes":".The {product} Dashboard supports OSDs provisioned by the `ceph-volume` utility\r\n\r\nIn this release, an update to the {product} Dashboard adds support for displaying information on `ceph-volume` provisioned OSDs.","short_desc":"Dashboard doesn't show disk metrics when cluster is provisioned with ceph-volume  (lvm)","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1600071,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:23:15Z","package_id":39656,"pm_score":0,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:23Z","release_notes":"","short_desc":"\"rbd-target-gw.service\" start failed with disk size mismatch error.","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1600613,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:23:15Z","package_id":39656,"pm_score":660,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:23Z","release_notes":"","short_desc":"[iscsi] rbd-target-gw service fails due to missing dependency python-flask RPM","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1600943,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:23:15Z","package_id":39207,"pm_score":730,"priority":"medium","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:23Z","release_notes":".Upgrading {product} 2 to version 3 will set the `sortbitwise` option properly\r\n\r\nPreviously, a rolling upgrade from {product} 2 to {product} 3 would fail because the OSDs would never initialize. This is because `sortbitwise` was not properly set by Ceph Ansible. With this release, Ceph Ansible sets `sortbitwise` properly, so the ODSs can start.","short_desc":"[cee/sd] upgrade RHCS 2 -> RHCS 3 will fail if cluster has still set sortnibblewise","verified":"FailedQA","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1601068,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"CodeChange","last_updated":"2018-09-26T18:23:15Z","package_id":39676,"pm_score":630,"priority":"medium","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:23Z","release_notes":"","short_desc":"rgw_gc_list may fail to report end of list","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1601325,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:23:15Z","package_id":39207,"pm_score":660,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:23Z","release_notes":"","short_desc":"[ceph-ansible:iscsi] rbd-target-gw service is in failed state after the playbook run","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,needinfo?,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1601580,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"CodeChange, FutureFeature","last_updated":"2018-09-26T18:23:15Z","package_id":39474,"pm_score":1660,"priority":"high","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:23Z","release_notes":".Add option --trim-delay-ms in radosgw-admin sync error trim command - to limit the frequency of osd ops\r\n\r\nA \"trim delay\" option has been added to the \"radosgw-admin sync error trim\" command in Ceph Object Gateway multisite. Previously, many OMAP keys could have been deleted by the full operation, leading to potential for impact on client workload. With the new option, trimming can be requested with low client workload impact.","short_desc":"[RFE] Add option --trim-delay-ms in  radosgw-admin sync error trim command - to limit the frequency of osd ops","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1601949,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:23:15Z","package_id":39207,"pm_score":660,"priority":"high","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:23Z","release_notes":"","short_desc":"[ceph-ansible] Failure on TASK [igw_purge | deleting configured rbd devices] with non-default clustername","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1602158,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:23:15Z","package_id":39656,"pm_score":0,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:23Z","release_notes":"","short_desc":"ceph iscsi: windows 2016 retries STPG even though success is returned","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,needinfo?,pm_ack+,qa_ack+","has_security_group":false,"id":1602327,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:23:15Z","package_id":39207,"pm_score":0,"priority":"low","qa_whiteboard":"","reconciled_at":"2018-09-26T18:25:23Z","release_notes":"","short_desc":"[cee/sd][ceph-ansible] for rbdmirrors in not copied clinet.admin keyring if \"copy_admin_key:true\" is specified in group_vars/rbd-mirrors.yml","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1602785,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:24:34Z","package_id":39207,"pm_score":1660,"priority":"high","qa_whiteboard":"","reconciled_at":"2018-09-26T18:26:30Z","release_notes":".Ceph ceph-ansible now installs the gwcli command during iscsi-gw install\r\n\r\nPreviously, when using Ansible playbooks from `ceph-ansible` to configure an iSCSI target, the `gwcli` command needed to verify the installation was not available. This was because the `ceph-iscsi-cli` package, which provides the `gwcli` command, was not included as a part of the install for the Ansible playbooks. With this update to {product}, the Ansible playbooks now install the `ceph-iscsi-cli` package as a part of iSCSI target configuration.","short_desc":"Ceph ceph-ansible does not install the gwcli command during iscsi-gw install","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1602882,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:24:34Z","package_id":39676,"pm_score":1720,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2018-09-26T18:26:30Z","release_notes":".Ceph is now able to delete/remove swift ACLs\r\n\r\nPreviously, the Swift CLI client could be used to set, but not to delete ACLs because the Swift header parsing logic could not detect ACL delete requests. With this update to {product}, the header parsing logic has been fixed, and users can delete ACLs with the Swift client.","short_desc":"Ceph Unable to delete/remove swift ACL's","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1607970,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:24:34Z","package_id":39207,"pm_score":0,"priority":"medium","qa_whiteboard":"","reconciled_at":"2018-09-26T18:26:30Z","release_notes":"","short_desc":"create an empty rados index object expects client.admin","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1608863,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:24:34Z","package_id":42766,"pm_score":660,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2018-09-26T18:26:31Z","release_notes":"","short_desc":"[cephmetrics] Cephmetrics ansible playbook fails for user define cluster name","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1609030,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:24:34Z","package_id":39676,"pm_score":0,"priority":"low","qa_whiteboard":"","reconciled_at":"2018-09-26T18:26:31Z","release_notes":"","short_desc":"rgw: fail to recover index from crash","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1609427,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-10-04T19:29:06Z","package_id":39681,"pm_score":2520,"priority":"urgent","qa_whiteboard":"","reconciled_at":"2018-10-04T19:30:33Z","release_notes":".The SELinux context is set correctly when using `ceph-volume` for new filesystems\r\n\r\nThe `ceph-volume` utility was not labeling newly created filesystems, which was causing `AVC` denial messages in the `/var/log/audit/audit.log` file. In this release, the `ceph-volume` utility sets the proper SELinux context (`ceph_var_lib_t`), on the OSD filesystem.","short_desc":"ceph-volume does not set SELinux context of newly mounted filesystems","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1610213,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:24:34Z","package_id":39207,"pm_score":0,"priority":"low","qa_whiteboard":"","reconciled_at":"2018-09-26T18:26:31Z","release_notes":"","short_desc":"Allow mgr bootstrap keyring to be defined","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1610220,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:24:34Z","package_id":39207,"pm_score":0,"priority":"low","qa_whiteboard":"","reconciled_at":"2018-09-26T18:26:31Z","release_notes":"","short_desc":"config: enforce socket name","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"automate_bug+,ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,qe_test_coverage+","has_security_group":false,"id":1610997,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:24:34Z","package_id":39207,"pm_score":2520,"priority":"urgent","qa_whiteboard":"","reconciled_at":"2018-09-26T18:26:31Z","release_notes":"","short_desc":"Unable to install ceph-common during upgrade from 2.5  to 3.1 on xenial","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1612854,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:24:34Z","package_id":39207,"pm_score":0,"priority":"low","qa_whiteboard":"","reconciled_at":"2018-09-26T18:26:31Z","release_notes":"","short_desc":"osd restart is limited to 99 OSD number","verified":"SanityOnly","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1613155,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:24:47Z","package_id":39207,"pm_score":660,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2018-09-26T18:26:31Z","release_notes":".Setting the `mon_use_fqdn` or the `mds_use_fqdn` options to `true` fails the Ceph Ansible playbook\r\n\r\nStarting with {product} 3.1, Red Hat no longer supports deployments with fully qualified domain names. If either the `mon_use_fqdn` or `mds_use_fqdn` options are set to `true`, then the Ceph Ansible playbook will fail. If the storage cluster is already configured with fully qualified domain names, then you must set the `use_fqdn_yes_i_am_sure` option to `true` in the `group_vars/all.yml` file.","short_desc":"[ceph-ansible] Do not allow ceph cluster creation when mon_use_fqdn and mds_use_fqdn set to true","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1613626,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"Regression","last_updated":"2018-09-26T18:24:47Z","package_id":39207,"pm_score":2160,"priority":"high","qa_whiteboard":"","reconciled_at":"2018-09-26T18:26:31Z","release_notes":".Containerized OSDs for which `osd_auto_discovery` flag was set to `true` properly restart during a rolling update \r\n\r\nPreviously, when using the Ansible rolling update playbook in a containerized environment, OSDs for which `osd_auto_discovery` flag is set to `true` are not restarted and the OSD services run with old image. With this release, the OSDs are restarting as expected.","short_desc":"[ceph-ansible] - rolling_update not upgrading containerized OSDs when osd_auto_discovery set to true","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1613963,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:24:47Z","package_id":39207,"pm_score":300,"priority":"medium","qa_whiteboard":"","reconciled_at":"2018-09-26T18:26:31Z","release_notes":"","short_desc":"igw: Start and enabled rbd-target-api","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1614286,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:24:47Z","package_id":39656,"pm_score":0,"priority":"low","qa_whiteboard":"","reconciled_at":"2018-09-26T18:26:31Z","release_notes":"","short_desc":"[iSCSI-gwcli] disk help create command needs change.","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1614287,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:24:47Z","package_id":39656,"pm_score":330,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2018-09-26T18:26:31Z","release_notes":"","short_desc":"[iscsi-gwcli]In help export command add description","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1615329,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:24:47Z","package_id":39656,"pm_score":630,"priority":"medium","qa_whiteboard":"","reconciled_at":"2018-09-26T18:26:31Z","release_notes":"","short_desc":"[gwcli] info command in client path should display 'nochap' auth type","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1615330,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:24:47Z","package_id":39656,"pm_score":330,"priority":"medium","qa_whiteboard":"","reconciled_at":"2018-09-26T18:26:31Z","release_notes":"","short_desc":"[iscsi-gwcli] help commands should use a consistent name for image_id","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1618678,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"Automation, AutomationBlocker, Regression","last_updated":"2018-09-26T18:24:47Z","package_id":39207,"pm_score":2820,"priority":"high","qa_whiteboard":"","reconciled_at":"2018-09-26T18:26:31Z","release_notes":".Ceph installation no longer fails when trying to deploy the Object Gateway\r\n\r\nWhen deploying the Ceph Object Gateway using Ansible, the `rgw_hostname` variable was not being set on the Object Gateway node, but was incorrectly set on the Ceph Monitor node. In this release, the `rgw_hostname` variable is set properly and applied to the Ceph Object Gateway node.","short_desc":"Installation fails with error: 'dict object' has no attribute 'rgw_hostname'","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","has_security_group":false,"id":1619098,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"Regression","last_updated":"2018-09-26T18:24:47Z","package_id":39207,"pm_score":1320,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2018-09-26T18:26:31Z","release_notes":".Installing the Object Gateway no longer fails for container deployments\r\n\r\nWhen installing the Object Gateway into a container the following error was observed:\r\n\r\n----\r\nfatal: [aio1_ceph-rgw_container-fc588f0a]: FAILED! => {\"changed\": false,\r\n\"cmd\": \"ceph --cluster ceph -s -f json\", \"msg\": \"[Errno 2] No such file\r\nor directory\"\r\n----\r\n\r\nAn execution task failed because there was no `ceph-common` package installed. This Ansible task was delegated to a Ceph Monitor node, which allows the execution to happen in the correct order.","short_desc":"[Ceph-Ansible][Container] [Filestore] RGW Installation failed","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1619167,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"Regression","last_updated":"2018-09-26T18:24:47Z","package_id":39207,"pm_score":2220,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2018-09-26T18:26:31Z","release_notes":"","short_desc":"[iscsi-ansible] iscsigws.yml should be iscsi-gws.yml","verified":"FailedQA","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1619189,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"Automation","last_updated":"2018-09-26T18:24:50Z","package_id":39474,"pm_score":660,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2018-09-26T18:26:31Z","release_notes":"","short_desc":"A period pull occasionally raises \"curl_easy_perform returned status 28 error: Operation too slow\"","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1619255,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:24:50Z","package_id":39207,"pm_score":330,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2018-09-26T18:26:31Z","release_notes":"","short_desc":"set_fact rule_name before luminous: 'dict object' has no attribute u'dummy'","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1622210,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:24:50Z","package_id":39207,"pm_score":660,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2018-09-26T18:26:31Z","release_notes":"","short_desc":"[iscsi-ansible] The playbook fails with unsupported system","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"automate_bug+,ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1622505,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"Automation","last_updated":"2018-10-17T11:20:48Z","package_id":39207,"pm_score":1920,"priority":"high","qa_whiteboard":"","reconciled_at":"2018-10-17T11:36:00Z","release_notes":"","short_desc":"Playbook for cluster setup with multiple RGW instances fails.","verified":"FailedQA","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1623417,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"Reopened","last_updated":"2018-09-26T18:24:50Z","package_id":39207,"pm_score":0,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2018-09-26T18:26:31Z","release_notes":"","short_desc":"ceph-ansible breaks keyrings created from old mon_cap, osd_cap, ... params","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1623737,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:24:50Z","package_id":39656,"pm_score":1320,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2018-09-26T18:26:31Z","release_notes":"","short_desc":"ceph-iscsi: clear internal lock state when path is revalidated","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1625191,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:24:50Z","package_id":39207,"pm_score":0,"priority":"high","qa_whiteboard":"","reconciled_at":"2018-09-26T18:26:31Z","release_notes":"","short_desc":"ceph-nfs runs rados commands on bare metal in containerized deployment","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","approved_release":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-3.y+,devel_ack+,pm_ack+,qa_ack+","has_security_group":false,"id":1630447,"internal_target_release":"","is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2018-09-26T18:24:50Z","package_id":39207,"pm_score":1320,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2018-09-26T18:26:31Z","release_notes":"","short_desc":"Deploying a cluster with multiple rgw instances breaks the ceph.conf","verified":"","was_marked_on_qa":1}}],"to_fetch":[]},"jira_issues":{"errata":{"rhba":{"actual_ship_date":"2018-09-26T18:17:30Z","assigned_to_id":3002003,"batch_id":null,"closed":0,"content_types":["rpm"],"contract":null,"created_at":"2018-05-03T22:40:06Z","current_state_index_id":233980,"current_tps_run":null,"deleted":0,"devel_responsibility_id":3,"doc_complete":1,"docs_responsibility_id":1,"embargo_undated":false,"filelist_changed":0,"filelist_locked":0,"fulladvisory":"RHBA-2018:2819-06","group_id":860,"id":33840,"is_batch_blocker":false,"is_brew":1,"is_valid":1,"issue_date":"2018-09-26T18:04:00Z","mailed":0,"manager_id":3001931,"old_advisory":"RHBA-2018:33840-06","old_delete_product":null,"package_owner_id":3001865,"priority":"normal","product_id":104,"publish_date_override":"2018-09-26T00:00:00Z","published":1,"published_shadow":0,"pushcount":1,"pushed":1,"qa_complete":1,"quality_responsibility_id":154,"rating":0,"release_date":null,"reporter_id":3001865,"request":0,"request_rcm_push_comment_id":null,"resolution":"","respin_count":10,"revision":6,"rhn_complete":0,"rhnqa":1,"rhnqa_shadow":0,"security_approved":null,"security_impact":"None","security_sla":"2018-10-01T00:00:00Z","severity":"normal","sign_requested":0,"state_machine_rule_set_id":null,"status":"SHIPPED_LIVE","status_updated_at":"2018-09-26T18:17:30Z","supports_multiple_product_destinations":true,"synopsis":"Red Hat Ceph Storage 3.1 Bug Fix update","text_only":false,"text_ready":0,"update_date":"2018-09-26T18:04:00Z","updated_at":"2018-09-26T18:25:11Z","embargo_date":null,"errata_id":2819,"publish_date":"2018-09-26T00:00:00Z"}},"id_field":"key","id_prefix":"jira:","type":"jira_issues","idsfixed":[],"jira_issues":[],"to_fetch":[]}}